{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Fc3OXuDeqP"
      },
      "source": [
        "# **Install InsightFace and Dependencies:**\n",
        "\n",
        "\n",
        "\n",
        " Run the following code to install the required packages.\n",
        "\n",
        "\n",
        "\n",
        " Note: This installation varies and depends on your cuda and cudnn version. Incase of face swapping issue refer to the link in the debug session to know the version to install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D-UgHjSFBE9d",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "6517ccd3-5483-4c1e-ef01-b8097acf3ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
            "Collecting onnxruntime-gpu==1.18.0\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/onnxruntime-gpu/1.18/onnxruntime_gpu-1.18.0-cp310-cp310-manylinux_2_28_x86_64.whl (200.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime-gpu==1.18.0)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/coloredlogs/15.0.1/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.18.0) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.18.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.18.0) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.18.0) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu==1.18.0)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/humanfriendly/10/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu==1.18.0) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.18.0\n",
            "Collecting insightface\n",
            "  Downloading insightface-0.7.3.tar.gz (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from insightface) (1.26.4)\n",
            "Collecting onnx (from insightface)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from insightface) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from insightface) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from insightface) (3.8.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from insightface) (11.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from insightface) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from insightface) (1.5.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from insightface) (0.24.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from insightface) (1.13)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from insightface) (3.0.11)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from insightface) (1.4.20)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from insightface) (3.12.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations->insightface) (3.10.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->insightface) (4.25.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->insightface) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (2024.8.30)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations->insightface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations->insightface) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations->insightface) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.16.0)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: insightface\n",
            "  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for insightface: filename=insightface-0.7.3-cp310-cp310-linux_x86_64.whl size=1055394 sha256=bdd0dd46921c0acd5fc02a2890a004920d60fe7f14d8aab9d277957a35723d67\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/d0/80/e3773fb8b6d1cca87ea1d33d9b1f20a223a6493c896da249b5\n",
            "Successfully built insightface\n",
            "Installing collected packages: onnx, insightface\n",
            "Successfully installed insightface-0.7.3 onnx-1.17.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "libavutil      56. 70.100 / 56. 70.100\n",
            "libavcodec     58.134.100 / 58.134.100\n",
            "libavformat    58. 76.100 / 58. 76.100\n",
            "libavdevice    58. 13.100 / 58. 13.100\n",
            "libavfilter     7.110.100 /  7.110.100\n",
            "libswscale      5.  9.100 /  5.  9.100\n",
            "libswresample   3.  9.100 /  3.  9.100\n",
            "libpostproc    55.  9.100 / 55.  9.100\n"
          ]
        }
      ],
      "source": [
        "#!pip install onnx==1.16.0\n",
        "!pip install onnxruntime-gpu==1.18.0 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
        "!pip install insightface #==0.7.3\n",
        "#!pip install onnxruntime==1.18.0\n",
        "!apt-get install -y ffmpeg\n",
        "!ffmpeg -version\n",
        "!mkdir deepfakecollab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYFbuuCdvTaO"
      },
      "source": [
        "### DeepFakeLive\n",
        "\n",
        "Follow the steps below to create the necessery directories for the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJJbUDokN0bF"
      },
      "outputs": [],
      "source": [
        "!cd deepfakecollab\n",
        "!mkdir deepfakecollab/Scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rTCd2R1uw7A"
      },
      "source": [
        "### [Optional] FRP\n",
        "\n",
        "\n",
        "\n",
        "Follow the steps below to setup FRP. You will also need to host the FRPS on your VPS (free).\n",
        "\n",
        "\n",
        "\n",
        "Note: You cannot run both FRP and ngrok together. Its ether FRP and ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-11-11T16:35:36.013863Z",
          "iopub.status.busy": "2024-11-11T16:35:36.013401Z",
          "iopub.status.idle": "2024-11-11T16:35:37.835343Z",
          "shell.execute_reply": "2024-11-11T16:35:37.833430Z",
          "shell.execute_reply.started": "2024-11-11T16:35:36.013814Z"
        },
        "id": "ckp_dM52N9K3",
        "outputId": "daba81ca-d00c-49b9-860f-245e5a7bcb01",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frpc is not found, installing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!touch deepfakecollab/Scripts/get_frs.sh\n",
        "getfrs = \"\"\"#!/usr/bin/env bash\n",
        "# Check if frpc is installed\n",
        "command -v frpc >/dev/null 2>&1\n",
        "if [[ $? -ne 0 ]]; then\n",
        "    echo \"frpc is not found, installing...\"\n",
        "    wget -q -nc https://github.com/fatedier/frp/releases/download/v0.59.0/frp_0.59.0_linux_amd64.tar.gz\n",
        "    tar -xzf frp_0.59.0_linux_amd64.tar.gz\n",
        "    echo \"Done!\"\n",
        "fi\"\"\"\n",
        "with open('deepfakecollab/Scripts/get_frs.sh', 'w') as f:\n",
        "    f.write(getfrs)\n",
        "!touch deepfakecollab/Scripts/open_tunnel_frs.sh\n",
        "getfrs = \"\"\"#!/usr/bin/env bash\n",
        "cmd=\"frp_0.59.0_linux_amd64/frpc -c frp_0.59.0_linux_amd64/frpc.toml\"\n",
        "kill -9 $(ps aux | grep $cmd | awk '{print $2}') 2> /dev/null\n",
        "echo Opening tunnel\n",
        "$cmd\"\"\"\n",
        "with open('deepfakecollab/Scripts/open_tunnel_frs.sh', 'w') as f:\n",
        "    f.write(getfrs)\n",
        "!chmod +x deepfakecollab/Scripts/get_frs.sh\n",
        "!chmod +x deepfakecollab/Scripts/open_tunnel_frs.sh\n",
        "!deepfakecollab/Scripts/get_frs.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n-qESinvM22"
      },
      "source": [
        "### [Optional]  Ngrok\n",
        "\n",
        "\n",
        "\n",
        "Follow to Setup Ngrok.\n",
        "\n",
        "\n",
        "\n",
        "Note: You need an API key from Ngrok to use tcp for free you would need to add a billing details to their platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9U6x85DvTTV",
        "outputId": "0ebbbafb-0a09-40f1-9a23-39d2f3a0c1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ngrok is not found, installing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!touch deepfakecollab/Scripts/get_ngrok.sh\n",
        "getfrs = \"\"\"#!/usr/bin/env bash\n",
        "# Check if frpc is installed\n",
        "command -v frpc >/dev/null 2>&1\n",
        "if [[ $? -ne 0 ]]; then\n",
        "    echo \"ngrok is not found, installing...\"\n",
        "    wget -q -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "    tar -xzf ngrok-v3-stable-linux-amd64.tgz\n",
        "    echo \"Done!\"\n",
        "fi\"\"\"\n",
        "with open('deepfakecollab/Scripts/get_ngrok.sh', 'w') as f:\n",
        "    f.write(getfrs)\n",
        "!touch deepfakecollab/Scripts/open_tunnel_ngrok.sh\n",
        "getfrs = \"\"\"#!/usr/bin/env bash\n",
        "cmd=\"./ngrok start --all --config ngrok.conf\"\n",
        "kill -9 $(ps aux | grep $cmd | awk '{print $2}') 2> /dev/null\n",
        "echo Opening tunnel\n",
        "$cmd\"\"\"\n",
        "with open('deepfakecollab/Scripts/open_tunnel_ngrok.sh', 'w') as f:\n",
        "    f.write(getfrs)\n",
        "!chmod +x deepfakecollab/Scripts/get_ngrok.sh\n",
        "!chmod +x deepfakecollab/Scripts/open_tunnel_ngrok.sh\n",
        "!deepfakecollab/Scripts/get_ngrok.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVpDmuTmwtO9"
      },
      "source": [
        "Set your region\n",
        "\n",
        "\n",
        "\n",
        "Code | Region\n",
        "\n",
        "--- | ---\n",
        "\n",
        "us | United States\n",
        "\n",
        "eu | Europe\n",
        "\n",
        "ap | Asia/Pacific\n",
        "\n",
        "au | Australia\n",
        "\n",
        "sa | South America\n",
        "\n",
        "jp | Japan\n",
        "\n",
        "in | India"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM-UUANCwwqo"
      },
      "outputs": [],
      "source": [
        "# Set your region here in quotes\n",
        "# Paste your authtoken here in quotes\n",
        "authtoken = \"\"\n",
        "region = \"eu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmNGrasvwnVX"
      },
      "source": [
        "### Create Model Folder\n",
        "\n",
        "\n",
        "\n",
        "Create and download into the model folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU1ZXGlGw8Rb",
        "outputId": "94095d7a-05f6-4d48-8ff2-f9928957f149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-14 17:05:07--  https://github.com/facefusion/facefusion-assets/releases/download/models/inswapper_128_fp16.onnx\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/681153318/dc179e61-f264-4521-abf7-25e2322b2b6c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241114%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241114T170507Z&X-Amz-Expires=300&X-Amz-Signature=38cadc0abb170199c6d6ec4402698646060c5bfd644801d689cd903dda660bfd&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dinswapper_128_fp16.onnx&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-11-14 17:05:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/681153318/dc179e61-f264-4521-abf7-25e2322b2b6c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241114%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241114T170507Z&X-Amz-Expires=300&X-Amz-Signature=38cadc0abb170199c6d6ec4402698646060c5bfd644801d689cd903dda660bfd&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dinswapper_128_fp16.onnx&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277680638 (265M) [application/octet-stream]\n",
            "Saving to: ‘/content/deepfakecollab/Model/inswapper_128_fp16.onnx’\n",
            "\n",
            "inswapper_128_fp16. 100%[===================>] 264.82M   176MB/s    in 1.5s    \n",
            "\n",
            "2024-11-14 17:05:09 (176 MB/s) - ‘/content/deepfakecollab/Model/inswapper_128_fp16.onnx’ saved [277680638/277680638]\n",
            "\n",
            "--2024-11-14 17:05:09--  https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.210.114, 13.35.210.61, 13.35.210.66, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.210.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/47/8e/478ea6841e7ee9b6a18fcbd35a172c66f44ae30fcce68054ea545317cb889208/e2cd4703ab14f4d01fd1383a8a8b266f9a5833dacee8e6a79d3bf21a1b6be5ad?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GFPGANv1.4.pth%3B+filename%3D%22GFPGANv1.4.pth%22%3B&Expires=1731863109&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTg2MzEwOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ny84ZS80NzhlYTY4NDFlN2VlOWI2YTE4ZmNiZDM1YTE3MmM2NmY0NGFlMzBmY2NlNjgwNTRlYTU0NTMxN2NiODg5MjA4L2UyY2Q0NzAzYWIxNGY0ZDAxZmQxMzgzYThhOGIyNjZmOWE1ODMzZGFjZWU4ZTZhNzlkM2JmMjFhMWI2YmU1YWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=cKB%7EAbsKAQ6TJ8kKtJ7xWlWmp21Zas9u-VEAXbz8aBTaXtKmsyvy9WAj9ktMi0LnUmX3Yyr8SnJgvODt1eyDFYPEiJM-tlWQPtu%7EmtdZjaTOxANSd-jOFmH6uxXy0TidCXeUcO1pChErA0rNHUO%7EIARkWDPh-BbOWbY2-8%7EgG0jkqPYIISziTJHC3tUJofOaygMQXFZV9dyOpmYny12s6ufi4MBbzcpPGQMvKKBySMxMp3AflakNSiZyq8-MNH75XzorzLz9QXFp1K2l1HAqH0WbMGJ%7ERC3VCCyMPFzWnuZDdh3VoosmBTRHRuNGAh8qtPHVviFIVfFqP9NZr5RtSw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-11-14 17:05:09--  https://cdn-lfs.hf.co/repos/47/8e/478ea6841e7ee9b6a18fcbd35a172c66f44ae30fcce68054ea545317cb889208/e2cd4703ab14f4d01fd1383a8a8b266f9a5833dacee8e6a79d3bf21a1b6be5ad?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GFPGANv1.4.pth%3B+filename%3D%22GFPGANv1.4.pth%22%3B&Expires=1731863109&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTg2MzEwOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ny84ZS80NzhlYTY4NDFlN2VlOWI2YTE4ZmNiZDM1YTE3MmM2NmY0NGFlMzBmY2NlNjgwNTRlYTU0NTMxN2NiODg5MjA4L2UyY2Q0NzAzYWIxNGY0ZDAxZmQxMzgzYThhOGIyNjZmOWE1ODMzZGFjZWU4ZTZhNzlkM2JmMjFhMWI2YmU1YWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=cKB%7EAbsKAQ6TJ8kKtJ7xWlWmp21Zas9u-VEAXbz8aBTaXtKmsyvy9WAj9ktMi0LnUmX3Yyr8SnJgvODt1eyDFYPEiJM-tlWQPtu%7EmtdZjaTOxANSd-jOFmH6uxXy0TidCXeUcO1pChErA0rNHUO%7EIARkWDPh-BbOWbY2-8%7EgG0jkqPYIISziTJHC3tUJofOaygMQXFZV9dyOpmYny12s6ufi4MBbzcpPGQMvKKBySMxMp3AflakNSiZyq8-MNH75XzorzLz9QXFp1K2l1HAqH0WbMGJ%7ERC3VCCyMPFzWnuZDdh3VoosmBTRHRuNGAh8qtPHVviFIVfFqP9NZr5RtSw__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.68.87, 18.155.68.37, 18.155.68.34, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.68.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 348632874 (332M) [binary/octet-stream]\n",
            "Saving to: ‘/content/deepfakecollab/Model/GFPGANv1.4.pth’\n",
            "\n",
            "GFPGANv1.4.pth      100%[===================>] 332.48M  77.3MB/s    in 5.1s    \n",
            "\n",
            "2024-11-14 17:05:15 (64.7 MB/s) - ‘/content/deepfakecollab/Model/GFPGANv1.4.pth’ saved [348632874/348632874]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p deepfakecollab/Model\n",
        "!wget https://github.com/facefusion/facefusion-assets/releases/download/models/inswapper_128_fp16.onnx -P /content/deepfakecollab/Model\n",
        "!wget https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth -P /content/deepfakecollab/Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNHGT2FjjZiK"
      },
      "source": [
        "### [Ignore]  DeBuggin (Run only when its necessery)\n",
        "\n",
        "\n",
        "\n",
        "Debugging to ensure cuda was used. if not check the version of the cudnn, edit the install of the onnxruntime package to install the right version. https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQr-gWl6Ibw7",
        "outputId": "97f42b83-1804-4149-a8e4-02729b4932f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#define CUDNN_MAJOR 8\n",
            "#define CUDNN_MINOR 9\n",
            "#define CUDNN_PATCHLEVEL 6\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "/* cannot use constexpr here since this is a C-only file */\n"
          ]
        }
      ],
      "source": [
        "!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ9r-s_9Kdha",
        "outputId": "f12d74e7-105b-46c5-ba2b-5676a6591f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.18.0\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime as rt\n",
        "print(rt.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stwb4OHv_hs0",
        "outputId": "2cd1efb1-fde0-4ef5-b769-66c9cc5c85b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n",
            "Active providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        }
      ],
      "source": [
        "# Ensure the GPU providers are set explicitly\n",
        "ort_session = rt.InferenceSession(\n",
        "    \"/content/deepfakecollab/Model/inswapper_128_fp16.onnx\",\n",
        "    providers=[\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        ")\n",
        "# Verify the active provider again\n",
        "print(\"Active providers:\", ort_session.get_providers())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk5osj-z2gST",
        "outputId": "751fd02b-ba91-4ca2-89ca-dd0abbd2330a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX Runtime is using: GPU\n"
          ]
        }
      ],
      "source": [
        "from onnxruntime import get_device\n",
        "print(\"ONNX Runtime is using:\", get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtkP4SLy7g9S",
        "outputId": "7a40b77f-5791-4cfa-819b-1a69882a2ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Nov 14 17:05:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0              26W /  70W |    637MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTbmyO-CK-8b"
      },
      "source": [
        "### Create the Colab Server\n",
        "\n",
        "\n",
        "\n",
        "In your Google Colab notebook, set up a TCP server that will receive frames, process them using the FACE_SWAPPER model, and send back the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4xE5bpeosP_",
        "outputId": "217a9216-8bc2-4d7d-95e6-ad65602d39a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['CUDAExecutionProvider']\n",
            "download_path: /root/.insightface/models/buffalo_l\n",
            "Downloading /root/.insightface/models/buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 281857/281857 [00:04<00:00, 67837.45KB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n"
          ]
        }
      ],
      "source": [
        "import socket\n",
        "import cv2\n",
        "import numpy as np\n",
        "import insightface\n",
        "import threading\n",
        "import torch\n",
        "import onnxruntime\n",
        "from typing import Any\n",
        "from insightface.app.common import Face\n",
        "import matplotlib.pyplot as plt\n",
        "# Check if CUDA is available\n",
        "if 'CUDAExecutionProvider' in onnxruntime.get_available_providers():\n",
        "    providers = ['CUDAExecutionProvider']\n",
        "elif 'TensorrtExecutionProvider'  in onnxruntime.get_available_providers():\n",
        "    providers = ['TensorrtExecutionProvider']\n",
        "else:\n",
        "    providers = ['CPUExecutionProvider']\n",
        "print(providers)\n",
        "FACE_SWAPPER = None\n",
        "FACE_ANALYSER = insightface.app.FaceAnalysis(name='buffalo_l', providers=providers)\n",
        "FACE_ANALYSER.prepare(ctx_id=0, det_size=(640, 640))\n",
        "THREAD_LOCK = threading.Lock()\n",
        "Frame = np.ndarray[Any, Any]\n",
        "def get_face_swapper() -> Any:\n",
        "    global FACE_SWAPPER\n",
        "    with THREAD_LOCK:\n",
        "        if FACE_SWAPPER is None:\n",
        "            model_path = \"/content/deepfakecollab/Model/inswapper_128_fp16.onnx\"\n",
        "            FACE_SWAPPER = insightface.model_zoo.get_model(model_path, providers=['CUDAExecutionProvider'])\n",
        "    return FACE_SWAPPER\n",
        "def swap_face(source_face: Face, target_face: Face, temp_frame: Frame) -> Frame:\n",
        "    return get_face_swapper().get(temp_frame, target_face, source_face, paste_back=True)\n",
        "def get_face_analyser() -> Any:\n",
        "    #FACE_ANALYSER.prepare(ctx_id=0, det_size=(640, 640))\n",
        "    return FACE_ANALYSER\n",
        "def get_one_face(frame: Frame) -> Any:\n",
        "    face = get_face_analyser().get(frame)\n",
        "    try:\n",
        "        return min(face, key=lambda x: x.bbox[0])\n",
        "    except ValueError:\n",
        "        return None\n",
        "def get_many_faces(frame: Frame) -> Any:\n",
        "    try:\n",
        "        return get_face_analyser().get(frame)\n",
        "    except IndexError:\n",
        "        return None\n",
        "def process_frame(source_face: Face, temp_frame: Frame,manyface: bool) -> Frame:\n",
        "    if manyface:\n",
        "        many_faces = get_many_faces(temp_frame)\n",
        "        if many_faces:\n",
        "            for target_face in many_faces:\n",
        "                temp_frame = swap_face(source_face, target_face, temp_frame)\n",
        "    else:\n",
        "        target_face = get_one_face(temp_frame)\n",
        "        if target_face:\n",
        "            temp_frame = swap_face(source_face, target_face, temp_frame)\n",
        "    return temp_frame\n",
        "# Input and output ports for communication\n",
        "local_in_source = 5555\n",
        "local_in_temp = 5556\n",
        "local_out_frame = 5557"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FehUyVNvGGgZ"
      },
      "source": [
        "### [Optional] For Live Streaming\n",
        "\n",
        "For Live Streaming from webcam run this cell but for just image swap run the next cell.\n",
        "\n",
        "\n",
        "\n",
        "Note: Don't run both cells at same time. You should either run this or the one below\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYQRVPsdxizn",
        "outputId": "9f843cfe-af7b-49d0-e094-30ff66ed3093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PULL one socket bound to tcp://127.0.0.1:5555\n",
            "OutputStream ffmpeg bound to tcp://127.0.0.1:5557?listen\n",
            "InputStream ffmpeg bound from tcp://127.0.0.1:5556?listen\n"
          ]
        }
      ],
      "source": [
        "import zmq\n",
        "import threading\n",
        "import cv2\n",
        "import numpy as np\n",
        "import msgpack\n",
        "import queue\n",
        "import time\n",
        "import zlib\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "from collections import deque\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from collections import deque\n",
        "\n",
        "# Initialize the executor with the number of workers\n",
        "executor = ThreadPoolExecutor(max_workers=4)  # Adjust based on your CPU cores\n",
        "# Batch size - tune this based on available resources and processing time per frame\n",
        "BATCH_SIZE = 120\n",
        "# Holds future results for parallel processing\n",
        "future_to_batch = {}\n",
        "#import matplotlib.pyplot as plt\n",
        "def create_demo_image():\n",
        "    # Create a demo image (e.g., a solid color or pattern)\n",
        "    demo_image = np.zeros((540, 960, 3), dtype=np.uint8)  # Black image\n",
        "    cv2.putText(demo_image, 'Demo Image', (50, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    return demo_image\n",
        "def pull_socket(local_in_port):\n",
        "    context = zmq.Context()\n",
        "    socket = context.socket(zmq.REP)\n",
        "    socket.setsockopt(zmq.RCVHWM, 100000)\n",
        "    socket.setsockopt(zmq.LINGER, 0)\n",
        "    address = f\"tcp://127.0.0.1:{local_in_port}\"\n",
        "    socket.bind(address)  # Binding to a different local port\n",
        "    print(f\"PULL one socket bound to {address}\")\n",
        "    return socket\n",
        "# Compress image\n",
        "def compress_image(image, quality=95):\n",
        "    # Set the JPEG quality parameter\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
        "    # Encode the image as a JPEG\n",
        "    result, encimg = cv2.imencode('.jpg', image, encode_param)\n",
        "    if not result:\n",
        "        raise Exception(\"Image encoding failed\")\n",
        "    # Decode the encoded image back to an image format\n",
        "    decimg = cv2.imdecode(encimg, 1)\n",
        "    return decimg\n",
        "# Decompress image\n",
        "def decompress_image(encimg):\n",
        "    image = cv2.imdecode(np.frombuffer(encimg, np.uint8), cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "# Global variables\n",
        "frames_array=deque(maxlen=2000)\n",
        "temp_frames_array=deque(maxlen=2000)\n",
        "source_frame = None\n",
        "is_manyFace = None\n",
        "frameSize = '960x540'\n",
        "fps = None\n",
        "# Process a batch of frames simultaneously\n",
        "# Function to process each frame\n",
        "def process_frame_parallel(source, frame, many_face):\n",
        "    return process_frame(get_one_face(source), frame, many_face)\n",
        "\n",
        "def pull_worker(pull_socket):\n",
        "    global source_frame,is_manyFace,frameSize,fps\n",
        "    while True:\n",
        "        try:\n",
        "            # Receive the JSON with total chunks\n",
        "            meta_data_json = pull_socket.recv_json()\n",
        "            #print(meta_data_json)\n",
        "            total_chunk = meta_data_json['total_chunk']\n",
        "            # Send acknowledgment for metadata\n",
        "            pull_socket.send_string(\"ACK\")\n",
        "            # Receive the array bytes\n",
        "            source_array_bytes =b''\n",
        "            for i in range(total_chunk):\n",
        "                chunk = pull_socket.recv()\n",
        "                source_array_bytes += chunk\n",
        "                pull_socket.send_string(f\"ACK {i + 1}/{total_chunk}\")\n",
        "            end_message = pull_socket.recv()\n",
        "            if end_message == b\"END\":\n",
        "                pull_socket.send_string(\"Final ACK\")\n",
        "\n",
        "            # Deserialize the bytes back to an ndarray\n",
        "            source_array = np.frombuffer(source_array_bytes, dtype=np.dtype(meta_data_json['dtype_source'])).reshape(meta_data_json['shape_source'])\n",
        "            #plt.imshow(source_array[:, :, ::-1])\n",
        "            #plt.show()\n",
        "            #frame_queue.append([\"source\", source_array])\n",
        "            source_frame = source_array\n",
        "            is_manyFace = meta_data_json['manyface']\n",
        "            frameSize =  meta_data_json['size']\n",
        "            fps = meta_data_json[\"fps\"]\n",
        "                #process_queue.put((\"source\", source_array))\n",
        "            break\n",
        "        except zmq.Again:\n",
        "            # Sleep briefly to avoid busy-waiting\n",
        "            time.sleep(0.01)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "def pull_worker_two(local_in_temp):\n",
        "    ffmpeg_receive_command =  [\n",
        "        'ffmpeg',\n",
        "        '-i',f'tcp://127.0.0.1:{local_in_temp}?listen&fifo_size=100000&overrun_nonfatal=1',\n",
        "        '-f','rawvideo',\n",
        "        '-pix_fmt','bgr24',\n",
        "        '-s','960x540',\n",
        "        'pipe:1'\n",
        "    ]\n",
        "    ffmpeg_receive_process = subprocess.Popen((ffmpeg_receive_command), stdout=subprocess.PIPE)\n",
        "    timefame =1/25\n",
        "    print(f\"InputStream ffmpeg bound from tcp://127.0.0.1:{local_in_temp}?listen\")\n",
        "    global source_frame,is_manyFace\n",
        "    future_to_frame = {}\n",
        "    while True:\n",
        "        try:\n",
        "            # Receive the JSON with total chunks\n",
        "            # Read decoded frame from FFmpeg\n",
        "            raw_frame = ffmpeg_receive_process.stdout.read(960 * 540 * 3)\n",
        "            if not raw_frame:\n",
        "                break\n",
        "            framex = np.frombuffer(raw_frame, dtype=np.uint8).reshape((540, 960, 3))\n",
        "            #print(framex)\n",
        "\n",
        "            source_array = source_frame\n",
        "            is_many_face = is_manyFace\n",
        "            temp_frames_array.append(framex)\n",
        "            #Clear the batch after submission\n",
        "            #latest_frame = temp_frames_array[-1]\n",
        "            # Submit processing tasks in parallel\n",
        "            #future = executor.submit(process_frame_parallel, source_frame, framex, is_manyFace)\n",
        "            ##future_to_frame[future] = framex\n",
        "            #'''\n",
        "            if source_frame is not None:\n",
        "\n",
        "\n",
        "                while len(temp_frames_array)>1:\n",
        "                    t_frames = temp_frames_array[-1]\n",
        "                    #processed_array = process_frame(get_one_face(source_frame),t_frames,is_manyFace)\n",
        "                    #plt.imshow(processed_array[:, :, ::-1])\n",
        "                    #plt.show()\n",
        "                    frames_array.append(t_frames)\n",
        "                    temp_frames_array.popleft()\n",
        "                '''\n",
        "            # Collect completed frames\n",
        "            for future in as_completed(future_to_frame):\n",
        "                processed_array = future.result()  # Get the processed frame\n",
        "                frames_array.append(processed_array)  # Store the processed frame\n",
        "                del future_to_frame[future]  # Clean up\n",
        "            '''\n",
        "            #time.sleep(timefame)\n",
        "        except zmq.Again:\n",
        "            # Sleep briefly to avoid busy-waiting\n",
        "            time.sleep(0.01)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "def push_worker(local_out_frame):\n",
        "    source_array = None\n",
        "    temp_array = None\n",
        "    is_many_face = None\n",
        "    timefrate = 120\n",
        "    maxcount = 0\n",
        "    global source_frame,is_manyFace,frameSize,fps\n",
        "    print(f\"OutputStream ffmpeg bound to tcp://127.0.0.1:{local_out_frame}?listen\")\n",
        "    ffmpeg_encode_command = [\n",
        "        'ffmpeg',\n",
        "        '-fflags','+nobuffer',\n",
        "        '-flags', 'low_delay',\n",
        "        '-probesize', '32',\n",
        "        '-f', 'rawvideo',\n",
        "        '-pix_fmt', 'bgr24',\n",
        "        '-s', '960x540',\n",
        "        '-r', str(timefrate),\n",
        "        '-i', 'pipe:',\n",
        "        '-c:v', 'libx264',  # Enable encoding for lower bandwidth usage\n",
        "        '-preset', 'ultrafast',  # Fast encoding to reduce latency\n",
        "        '-tune', 'zerolatency',  # Low latency setting\n",
        "        '-g', '60',  # GOP size set to 1 to make each frame a keyframe\n",
        "        '-b:v', '3000k',  # Set bitrate to 1 Mbps\n",
        "        '-maxrate', '3000k',\n",
        "        '-bufsize', '5000k',\n",
        "        '-threads', '8',  # Enable threading\n",
        "        '-f', 'mpegts',\n",
        "        f'tcp://127.0.0.1:{local_out_frame}?listen&fifo_size=50000&overrun_nonfatal=1'\n",
        "    ]\n",
        "    timefame =1/timefrate\n",
        "    ffmpeg_encode_process = subprocess.Popen((ffmpeg_encode_command), stdin=subprocess.PIPE)\n",
        "    demo_image = create_demo_image()\n",
        "    frames_to_skip = 200  # Number of frames to skip to reduce delay\n",
        "    frame_count =0\n",
        "    wait_frame = 0\n",
        "    try:\n",
        "        while True:\n",
        "            # Get the processed array and metadata from the queue\n",
        "            if len(frames_array)>wait_frame:\n",
        "                '''\n",
        "                while len(frames_array)>7:#frame_count<frames_to_skip:\n",
        "                    frames_array.popleft()\n",
        "                    frame_count+=1\n",
        "                if len(frames_array)==7:\n",
        "                    wait_frame = 5\n",
        "                '''\n",
        "                if wait_frame<timefrate:\n",
        "                    wait_frame += 1\n",
        "                    #timefrate -=1\n",
        "\n",
        "                temp_array = frames_array[-1]#frames_array.popleft()#process_queue.get()\n",
        "                source_array = source_frame\n",
        "                is_many_face = is_manyFace\n",
        "                #framex = temp_array\n",
        "                #print(source_array,temp_array)\n",
        "                if source_frame is not None: #and temp_array is not None:\n",
        "                    processed_array = process_frame(get_one_face(source_frame),temp_array,is_manyFace)\n",
        "                    framex = processed_array\n",
        "                    #print(\"frames_array,\",len(frames_array))\n",
        "                    #processed_array_bytes = temp_array.tobytes()#processed_array.tobytes()#temp_array.tobytes()\n",
        "                    #ffmpeg_encode_process.stdin.write(processed_array_bytes)\n",
        "                    #push_socket.send(zlib.compress(processed_array_bytes))\n",
        "                    source_array = None\n",
        "                    temp_array = None\n",
        "                    is_many_face = None\n",
        "            else:\n",
        "                framex = demo_image\n",
        "                # Write the frame to FFmpeg for encoding and streaming\n",
        "            ffmpeg_encode_process.stdin.write(framex.tobytes())\n",
        "            time.sleep(timefame)\n",
        "    finally:\n",
        "    #ffmpeg_receive_process.terminate()\n",
        "        ffmpeg_encode_process.terminate()\n",
        "# Create sockets\n",
        "pull_socket = pull_socket(local_in_source)\n",
        "# Run both workers in separate threads\n",
        "# Start the pull worker thread\n",
        "pull_thread = threading.Thread(target=pull_worker, args=(pull_socket,))\n",
        "pull_thread.start()\n",
        "# Start the push worker thread\n",
        "pull_thread_two = threading.Thread(target=pull_worker_two, args=(local_in_temp,))\n",
        "pull_thread_two.start()\n",
        "# Start the push worker thread\n",
        "push_thread = threading.Thread(target=push_worker,args=(local_out_frame,))\n",
        "push_thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb_cbyoaHKrT"
      },
      "source": [
        "### [Optional] For Image Swap\n",
        "\n",
        "For image swapping run this cell.\n",
        "\n",
        "\n",
        "\n",
        "Note: You can not run both cell at same time. Either This or The cell Above (For Live Streaming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_nrCfTpHWS4",
        "outputId": "10038760-2787-44ed-83fb-35e74a757205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PULL one socket bound to tcp://127.0.0.1:5555\n",
            "PULL one socket bound to tcp://127.0.0.1:5556\n",
            "PUSH socket bound to tcp://127.0.0.1:5557\n"
          ]
        }
      ],
      "source": [
        "import zmq\n",
        "import threading\n",
        "import cv2\n",
        "import numpy as np\n",
        "import msgpack\n",
        "import queue\n",
        "import time\n",
        "import zlib\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "from collections import deque\n",
        "import matplotlib as plt\n",
        "def push_socket(local_out_port):\n",
        "    context = zmq.Context()\n",
        "    socket = context.socket(zmq.REQ)\n",
        "    socket.setsockopt(zmq.SNDHWM, 100000)\n",
        "    socket.setsockopt(zmq.LINGER, 0)\n",
        "    address = f\"tcp://127.0.0.1:{local_out_port}\"\n",
        "    socket.bind(address)  # Binding to a local port\n",
        "    print(f\"PUSH socket bound to {address}\")\n",
        "    return socket\n",
        "def pull_socket(local_in_port):\n",
        "    context = zmq.Context()\n",
        "    socket = context.socket(zmq.REP)\n",
        "    socket.setsockopt(zmq.RCVHWM, 100000)\n",
        "    socket.setsockopt(zmq.LINGER, 0)\n",
        "    address = f\"tcp://127.0.0.1:{local_in_port}\"\n",
        "    socket.bind(address)  # Binding to a different local port\n",
        "    print(f\"PULL one socket bound to {address}\")\n",
        "    return socket\n",
        "# Compress image\n",
        "def compress_image(image, quality=95):\n",
        "    # Set the JPEG quality parameter\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
        "    # Encode the image as a JPEG\n",
        "    result, encimg = cv2.imencode('.jpg', image, encode_param)\n",
        "    if not result:\n",
        "        raise Exception(\"Image encoding failed\")\n",
        "    # Decode the encoded image back to an image format\n",
        "    decimg = cv2.imdecode(encimg, 1)\n",
        "    return decimg\n",
        "# Decompress image\n",
        "def decompress_image(encimg):\n",
        "    image = cv2.imdecode(np.frombuffer(encimg, np.uint8), cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "# Global variables\n",
        "frames_array=deque(maxlen=2000)\n",
        "source_frame = None\n",
        "is_manyFace = None\n",
        "frameSize = '640x480'\n",
        "fps = None\n",
        "#functions\n",
        "def pull_worker(pull_socket):\n",
        "    global source_frame,is_manyFace,frameSize,fps\n",
        "    while True:\n",
        "        try:\n",
        "            # Receive the JSON with total chunks\n",
        "            meta_data_json = pull_socket.recv_json()\n",
        "            #print(meta_data_json)\n",
        "            total_chunk = meta_data_json['total_chunk']\n",
        "            # Send acknowledgment for metadata\n",
        "            pull_socket.send_string(\"ACK\")\n",
        "            # Receive the array bytes\n",
        "            source_array_bytes =b''\n",
        "            for i in range(total_chunk):\n",
        "                chunk = pull_socket.recv()\n",
        "                source_array_bytes += chunk\n",
        "                pull_socket.send_string(f\"ACK {i + 1}/{total_chunk}\")\n",
        "            end_message = pull_socket.recv()\n",
        "            if end_message == b\"END\":\n",
        "                pull_socket.send_string(\"Final ACK\")\n",
        "            # Deserialize the bytes back to an ndarray\n",
        "            source_array = np.frombuffer(source_array_bytes, dtype=np.dtype(meta_data_json['dtype_source'])).reshape(meta_data_json['shape_source'])\n",
        "            #plt.imshow(source_array[:, :, ::-1])\n",
        "            #plt.show()\n",
        "            #frame_queue.append([\"source\", source_array])\n",
        "            source_frame = source_array\n",
        "            is_manyFace = meta_data_json['manyface']\n",
        "            frames_array.append([\"source\",source_array,is_manyFace])\n",
        "                #process_queue.put((\"source\", source_array))\n",
        "            #break\n",
        "        except zmq.Again:\n",
        "            # Sleep briefly to avoid busy-waiting\n",
        "            time.sleep(0.01)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "def pull_worker_two(pull_socket_two):\n",
        "    while True:\n",
        "        try:\n",
        "           # Receive the JSON with total chunks\n",
        "            meta_data_json = pull_socket_two.recv_json()\n",
        "            #print(meta_data_json)\n",
        "            total_chunk = meta_data_json['total_chunk']\n",
        "            # Send acknowledgment for metadata\n",
        "            pull_socket_two.send_string(\"ACK\")\n",
        "            # Receive the array bytes\n",
        "            temp_array_bytes =b''\n",
        "            for i in range(total_chunk):\n",
        "                chunk = pull_socket_two.recv()\n",
        "                temp_array_bytes += chunk\n",
        "                pull_socket_two.send_string(f\"ACK {i + 1}/{total_chunk}\")\n",
        "            end_message = pull_socket_two.recv()\n",
        "            if end_message == b\"END\":\n",
        "                pull_socket_two.send_string(\"Final ACK\")\n",
        "            # Deserialize the bytes back to an ndarray\n",
        "            temp_array = np.frombuffer(temp_array_bytes, dtype=np.dtype(meta_data_json['dtype_temp'])).reshape(meta_data_json['shape_temp'])\n",
        "            #if source_frame is not None:\n",
        "            frames_array.append([\"temp\",temp_array])\n",
        "            print(\"added\",len(frames_array))\n",
        "            #break\n",
        "        except zmq.Again:\n",
        "            # Sleep briefly to avoid busy-waiting\n",
        "            time.sleep(0.01)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "def push_worker(push_socket):\n",
        "    global source_frame,is_manyFace,frameSize,fps\n",
        "    temp_frm = None\n",
        "    source_frm = None\n",
        "    is_manyface = None\n",
        "    try:\n",
        "        while True:\n",
        "            # Get the processed array and metadata from the queue\n",
        "            if len(frames_array)>0:\n",
        "                #print(frames_array)\n",
        "                item = frames_array.popleft()#process_queue.get()\n",
        "                if item[0]==\"source\":\n",
        "                    source_frm = item[1]\n",
        "                    is_manyface = item[2]\n",
        "                if item[0]==\"temp\":\n",
        "                    temp_frm = item[1]\n",
        "                    print(\"Recieved\")\n",
        "                if temp_frm is not None and source_frm is not None:\n",
        "                    processed_frm =process_frame(get_one_face(source_frm),temp_frm,is_manyface)\n",
        "                    face_bytes = processed_frm.tobytes()\n",
        "                    chunk_size = 1024*200\n",
        "                    total_chunk = len(face_bytes) // chunk_size + 1\n",
        "                    metadata ={\n",
        "                        'dtype_source':str(processed_frm.dtype),\n",
        "                        'shape_source':processed_frm.shape,\n",
        "                        'size':'640x480',\n",
        "                        'fps':'60'\n",
        "                        #'shape_temp':temp_frame.shape\n",
        "                    }\n",
        "                    new_metadata = {'total_chunk': total_chunk}\n",
        "                    metadata.update(new_metadata)\n",
        "                    # Send metadata first\n",
        "                    push_socket.send_json(metadata)\n",
        "                    # Wait for acknowledgment for metadata\n",
        "                    ack = push_socket.recv_string()\n",
        "                    with tqdm(total=total_chunk, desc=\"Sending chunks\", unit=\"chunk\") as pbar:\n",
        "                        for i in range(total_chunk):\n",
        "                            chunk = face_bytes[i * chunk_size:(i + 1) * chunk_size]\n",
        "                            # Send the chunk\n",
        "                            push_socket.send(chunk)\n",
        "                            # Wait for acknowledgment after sending each chunk\n",
        "                            ack = push_socket.recv_string()\n",
        "                            pbar.set_postfix_str(f'Chunk {i + 1}/{total_chunk} ack: {ack}')\n",
        "                            pbar.update(1)\n",
        "                    # Send a final message to indicate all chunks are sent\n",
        "                    push_socket.send(b\"END\")\n",
        "                    # Wait for the final reply\n",
        "                    final_reply_message = push_socket.recv_string()\n",
        "                    print(f\"Received final reply: {final_reply_message}\")\n",
        "                    temp_frm = None\n",
        "                    source_frm = None\n",
        "                    is_manyface = None\n",
        "    except Exception as e:\n",
        "        print (f\"Error in Push Wokr {e}\")\n",
        "local_in_source = 5555\n",
        "local_in_temp = 5556\n",
        "local_out_frame = 5557\n",
        "# Create sockets\n",
        "pull_socket_ = pull_socket(local_in_source)\n",
        "pull_socket_two = pull_socket(local_in_temp)\n",
        "push_socket_ = push_socket(local_out_frame)\n",
        "# Run both workers in separate threads\n",
        "# Start the pull worker thread\n",
        "pull_thread = threading.Thread(target=pull_worker, args=(pull_socket_,))\n",
        "pull_thread.start()\n",
        "# Start the push worker thread\n",
        "pull_thread_two = threading.Thread(target=pull_worker_two, args=(pull_socket_two,))\n",
        "pull_thread_two.start()\n",
        "# Start the push worker thread\n",
        "push_thread = threading.Thread(target=push_worker,args=(push_socket_,))\n",
        "push_thread.start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9JW1rhH3H-T"
      },
      "source": [
        "### [Optional]  Open FRP tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNDmeHcx3Zrx"
      },
      "outputs": [],
      "source": [
        "HostIP =194.113.64.71\n",
        "frpc_config = f\"\"\"serverAddr = \"{str(HostIP)}\"\n",
        "serverPort = 7000\n",
        "[[proxies]]\n",
        "name = \"Pull-tcp\"\n",
        "type = \"tcp\"\n",
        "localIP = \"127.0.0.1\"\n",
        "localPort = {local_in_source}\n",
        "remotePort = 6000\n",
        "[[proxies]]\n",
        "name = \"Pull-tcp_two\"\n",
        "type = \"tcp\"\n",
        "localIP = \"127.0.0.1\"\n",
        "localPort = {local_in_temp}\n",
        "remotePort = 6001\n",
        "[[proxies]]\n",
        "name = \"Push-tcp\"\n",
        "type = \"tcp\"\n",
        "localIP = \"127.0.0.1\"\n",
        "localPort = {local_out_frame}\n",
        "remotePort = 6002\"\"\"\n",
        "with open('frp_0.59.0_linux_amd64/frpc.toml', 'w') as f:\n",
        "    f.write(frpc_config)\n",
        "#Remote tcp\n",
        "print(f'tcp://{str(HostIP)}:6000 ---> {local_in_source}')\n",
        "print(f'tcp://{str(HostIP)}:6001 ---> {local_in_temp}')\n",
        "print(f'tcp://1{str(HostIP)}:6002 ---> {local_out_frame}')\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "import time\n",
        "ps = Popen('/kaggle/working/deepfakecollab/Scripts/open_tunnel_frs.sh', stdout=PIPE, stderr=PIPE)\n",
        "time.sleep(3)\n",
        "!/kaggle/working/deepfakecollab/Scripts/open_tunnel_frs.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buCnH_YpxKWa"
      },
      "source": [
        "### [Optional]  Open Ngrok tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyY2B5bmxlui",
        "outputId": "7b1bc4d4-3cc8-41c6-f374-9eb5016d6ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcp://5.tcp.eu.ngrok.io:16277 -> 5555 [input]\n",
            "tcp://4.tcp.eu.ngrok.io:14644 -> 5557 [output]\n",
            "tcp://2.tcp.eu.ngrok.io:10238 -> 5556 [inputtwo]\n",
            "Tunnel opened\n"
          ]
        }
      ],
      "source": [
        "from subprocess import Popen, PIPE\n",
        "import shlex\n",
        "import json\n",
        "import time\n",
        "def run_with_pipe(command):\n",
        "  commands = list(map(shlex.split,command.split(\"|\")))\n",
        "  ps = Popen(commands[0], stdout=PIPE, stderr=PIPE)\n",
        "  for command in commands[1:]:\n",
        "    ps = Popen(command, stdin=ps.stdout, stdout=PIPE, stderr=PIPE)\n",
        "  return ps.stdout.readlines()\n",
        "def get_tunnel_adresses():\n",
        "  info = run_with_pipe(\"curl http://localhost:4040/api/tunnels\")\n",
        "  assert info\n",
        "  info = json.loads(info[0])\n",
        "  for tunnel in info['tunnels']:\n",
        "    url = tunnel['public_url']\n",
        "    port = url.split(':')[-1]\n",
        "    local_port = tunnel['config']['addr'].split(':')[-1]\n",
        "    print(f'{url} -> {local_port} [{tunnel[\"name\"]}]')\n",
        "    if tunnel['name'] == 'input':\n",
        "      in_addr = url\n",
        "    elif tunnel['name'] == 'inputtwo':\n",
        "      in_addrtwo = url\n",
        "    elif tunnel['name'] == 'output':\n",
        "      out_addr = url\n",
        "    else:\n",
        "      print(f'unknown tunnel: {tunnel[\"name\"]}')\n",
        "  return in_addr,in_addrtwo, out_addr\n",
        "config =\\\n",
        "f\"\"\"\n",
        "version: 2\n",
        "authtoken: {authtoken}\n",
        "region: {region}\n",
        "console_ui: False\n",
        "tunnels:\n",
        "  input:\n",
        "    addr: {local_in_source}\n",
        "    proto: tcp\n",
        "  inputtwo:\n",
        "    addr: {local_in_temp}\n",
        "    proto: tcp\n",
        "  output:\n",
        "    addr: {local_out_frame}\n",
        "    proto: tcp\n",
        "\"\"\"\n",
        "with open('ngrok.conf', 'w') as f:\n",
        "  f.write(config)\n",
        "# (Re)Open tunnel\n",
        "ps = Popen('/content/deepfakecollab/Scripts/open_tunnel_ngrok.sh', stdout=PIPE, stderr=PIPE)\n",
        "time.sleep(3)\n",
        "# Get tunnel addresses\n",
        "try:\n",
        "  in_addr,in_addr_two, out_addr = get_tunnel_adresses()\n",
        "  print(\"Tunnel opened\")\n",
        "except Exception as e:\n",
        "  [print(l.decode(), end='') for l in ps.stdout.readlines()]\n",
        "  print(\"Something went wrong, reopen the tunnel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISPEVhDV3x3c"
      },
      "source": [
        "##  **KILL PROCESS (PORTS)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kC3axJEx5e8"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "# List of ports to free\n",
        "ports = [5555, 5556, 5557]\n",
        "for port in ports:\n",
        "    # Find the process ID (PID) using the port\n",
        "    result = subprocess.run(f\"lsof -t -i:{port}\", shell=True, capture_output=True, text=True)\n",
        "    pid = result.stdout.strip()\n",
        "\n",
        "    # Kill the process if it exists\n",
        "    if pid:\n",
        "        subprocess.run(f\"kill -9 {pid}\", shell=True)\n",
        "        print(f\"Terminated process on port {port} with PID {pid}\")\n",
        "    else:\n",
        "        print(f\"No process found on port {port}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sNHGT2FjjZiK",
        "Eb_cbyoaHKrT",
        "a9JW1rhH3H-T"
      ],
      "gpuType": "L4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
